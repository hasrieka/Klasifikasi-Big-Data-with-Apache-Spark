{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of Sentiment Analysis with MLlib.ipynb","provenance":[{"file_id":"1hdDKEYCjYod9pLVPZYYK25EcYiCcxzmF","timestamp":1622370316264},{"file_id":"1wFeAjI_wdOk0RZSsuTaNThT76SHYaywg","timestamp":1621759433201}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"P9lBAs9ZZ3fH"},"source":["# Install Package and Manage Data"]},{"cell_type":"code","metadata":{"id":"A5PW5dZnVT2-"},"source":["# !pip install Sastrawi\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"," \n","import re\n","\n","import random\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","from plotly import graph_objs as go\n","import plotly.express as px\n","import plotly.figure_factory as ff\n","from collections import Counter\n","# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n"," \n"," \n","import nltk\n","from nltk.corpus import stopwords\n"," \n","from tqdm import tqdm\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Kf-CISeZmxU","executionInfo":{"status":"ok","timestamp":1622347137858,"user_tz":-480,"elapsed":20964,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"545ef77b-2ae5-4d9b-a0d7-50f983c1b47d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HiGqkUyyXN22"},"source":["import tensorflow as tf\n","# df = pd.read_csv(\"/content/drive/MyDrive/train.csv\", usecols=[\"label\", \"reviewText\"])\n","# with tf.device('/device:GPU:0'):\n","#df = pd.read_csv(\"/content/drive/MyDrive/train.csv\", usecols=[\"label\", \"reviewText\"])\n","#import tensorflow as tf\n","df = pd.read_csv(\"/content/drive/MyDrive/Big Data Eka/Train1_stemmed_lemmatized_no_extra_stops.csv\", usecols=[\"label\", \"text_lemmatize\" ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"QgjmBTvoXRsY","executionInfo":{"status":"ok","timestamp":1622363108814,"user_tz":-480,"elapsed":20,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"20d62ee1-1368-49fc-bc13-c55b18ae84a8"},"source":["df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text_lemmatize</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>sound track beautiful paint senery mind well w...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>im reading lot review saying best game soundtr...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>soundtrack favorite music time hand intense sa...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>truly like soundtrack enjoy video game music p...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>youve played game know divine music every sing...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>399993</th>\n","      <td>1</td>\n","      <td>sure trainer super easy set holy god loud know...</td>\n","    </tr>\n","    <tr>\n","      <th>399994</th>\n","      <td>2</td>\n","      <td>sock aid cloth covered great hose sock tended ...</td>\n","    </tr>\n","    <tr>\n","      <th>399995</th>\n","      <td>1</td>\n","      <td>book frustrating put japanese text tiny letter...</td>\n","    </tr>\n","    <tr>\n","      <th>399996</th>\n","      <td>2</td>\n","      <td>came across book vacation cabin minnesota read...</td>\n","    </tr>\n","    <tr>\n","      <th>399997</th>\n","      <td>1</td>\n","      <td>found book beyond wanted get involved withespe...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>399998 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["        label                                     text_lemmatize\n","0           2  sound track beautiful paint senery mind well w...\n","1           2  im reading lot review saying best game soundtr...\n","2           2  soundtrack favorite music time hand intense sa...\n","3           2  truly like soundtrack enjoy video game music p...\n","4           2  youve played game know divine music every sing...\n","...       ...                                                ...\n","399993      1  sure trainer super easy set holy god loud know...\n","399994      2  sock aid cloth covered great hose sock tended ...\n","399995      1  book frustrating put japanese text tiny letter...\n","399996      2  came across book vacation cabin minnesota read...\n","399997      1  found book beyond wanted get involved withespe...\n","\n","[399998 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":187}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0I4XIZeXjMv","executionInfo":{"status":"ok","timestamp":1622363108814,"user_tz":-480,"elapsed":16,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"5525b1ef-5b55-48a6-b2f8-7c672a24166a"},"source":["df.isnull().sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["label             0\n","text_lemmatize    0\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":188}]},{"cell_type":"markdown","metadata":{"id":"_tw3dzKpjOXo"},"source":["# PYSPARK"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8vruXzLjjVgR","executionInfo":{"status":"ok","timestamp":1622363113164,"user_tz":-480,"elapsed":2770,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"3ec2c846-a71c-4b61-d347-df9a7383b336"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n","Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"srv-H6TRjNsz"},"source":["from pyspark.sql import Row\n","# from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n","from pyspark.ml.feature import CountVectorizer, StopWordsRemover, IDF\n","from pyspark.ml.feature import Tokenizer, RegexTokenizer\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.classification import NaiveBayes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qm_RCcuC7wwe"},"source":["# Menyiapkan Data"]},{"cell_type":"code","metadata":{"id":"7cie5cBjjfCZ"},"source":["from pyspark.context import SparkContext\n","from pyspark.sql.session import SparkSession\n","# sc = SparkContext.getOrCreate()\n","sc = SparkSession.builder.appName('SentimentClassifier').getOrCreate()\n","spark = SparkSession(sc)\n","# documents = sc.dataframe(\"/content/drive/MyDrive/train.csv\").map(lambda line: line.split(\" \"))\n","# save data https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html\n","dfSPARK = spark.read.load(\"/content/drive/MyDrive/Big Data Eka/Train1_stemmed_lemmatized_no_extra_stops.csv\",\n","                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dPBad6ZkMuO","executionInfo":{"status":"ok","timestamp":1622363825774,"user_tz":-480,"elapsed":307,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"bb7ac60c-4208-4c57-e6fa-9a2948d831cb"},"source":["datafix.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|label|          reviewText|       selected_text|         text_porter|       text_snowball|      text_lemmatize|\n","+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|    2|This sound track ...|sound track beaut...|sound track beaut...|sound track beaut...|sound track beaut...|\n","|    2|I'm reading a lot...|im reading lot re...|im read lot revie...|im read lot revie...|im reading lot re...|\n","|    2|\"This soundtrack ...| \"\"Time of the Dr...| and \"\"Chronomant...| probably the bes...| so I can't say f...|\n","|    2|I truly like this...|truly like soundt...|truli like soundt...|truli like soundt...|truly like soundt...|\n","|    2|If you've played ...|youve played game...|youv play game kn...|youv play game kn...|youve played game...|\n","|    2|I am quite sure a...|quite sure actual...|quit sure actual ...|quit sure actual ...|quite sure actual...|\n","|    1|\"This is a self-p...| unless you are i...| far away from th...|selfpublished boo...|selfpublish book ...|\n","|    2|I loved Whisper o...|loved whisper wic...|love whisper wick...|love whisper wick...|loved whisper wic...|\n","|    2|I just finished r...|finished reading ...|finish read whisp...|finish read whisp...|finished reading ...|\n","|    2|This was a easy t...|easy read book ma...|easi read book ma...|easi read book ma...|easy read book ma...|\n","|    1|A complete waste ...|complete waste ti...|complet wast time...|complet wast time...|complete waste ti...|\n","|    2|This was a great ...|great booki could...|great booki could...|great booki could...|great booki could...|\n","|    2|I thought this bo...|thought book bril...|thought book bril...|thought book bril...|thought book bril...|\n","|    1|I guess you have ...|guess romance nov...|guess romanc nove...|guess romanc nove...|guess romance nov...|\n","|    1|\"I feel I have to...| that I decided t...| believe that the...|feel write keep o...|feel write keep o...|\n","|    1|It's glaringly ob...|glaringly obvious...|glaringli obviou ...|glare obvious glo...|glaringly obvious...|\n","|    2|\"When you hear fo...| three hysterical...| often comical bu...| interaction with...| that salsa sound...|\n","|    2|After I had a DVT...|dvt doctor requir...|dvt doctor requir...|dvt doctor requir...|dvt doctor requir...|\n","|    2|Excellent product...|excellent product...|excel product how...|excel product how...|excellent product...|\n","|    1|sizes are much sm...|sizes much smalle...|size much smaller...|size much smaller...|size much smaller...|\n","+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V5oIgiwi7DqE"},"source":["# Tokenize Menggunakan Pyspark"]},{"cell_type":"code","metadata":{"id":"iuFYrOpCpUBX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622363906886,"user_tz":-480,"elapsed":485,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"12f76066-e603-4b94-d66d-82bf74edaacd"},"source":["# tokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"tokenText\")\n","# categoryIndexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n","\n","stages = []\n","# 1. clean data and tokenize sentences using RegexTokenizer\n","regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokens\")\n","stages += [regexTokenizer]\n","\n","# 2. CountVectorize the data\n","cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"token_features\", minDF=2.0)#, vocabSize=3, minDF=2.0\n","stages += [cv]\n","\n","# 3. Vectorise features using vectorassembler\n","vecAssembler = VectorAssembler(inputCols=['token_features'], outputCol=\"features\")\n","stages += [vecAssembler]\n","\n","# 4. Convert the labels to numerical values using binariser\n","indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelpredict\")\n","stages += [indexer]\n","\n","[print('\\n', stage) for stage in stages]\n","# wordsData = tokenizer.transform(df)\n","# regexTokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n","# wordsData = regexTokenizer.transform(df)\n","#https://stackoverflow.com/questions/41352267/creating-tf-idf-vector-from-a-spark-dataframe-with-text-column"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"," RegexTokenizer_8a02666b5b90\n","\n"," CountVectorizer_6f34d480554d\n","\n"," VectorAssembler_f8330d90d6ef\n","\n"," StringIndexer_476e8cc14bf2\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[None, None, None, None]"]},"metadata":{"tags":[]},"execution_count":224}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DcbF67wVSUkG","executionInfo":{"status":"ok","timestamp":1622363910340,"user_tz":-480,"elapsed":324,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"0f3600ff-6f78-4efe-fc4e-c2774323ef1a"},"source":["datafix.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|label|          reviewText|       selected_text|         text_porter|       text_snowball|      text_lemmatize|\n","+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|    2|This sound track ...|sound track beaut...|sound track beaut...|sound track beaut...|sound track beaut...|\n","|    2|I'm reading a lot...|im reading lot re...|im read lot revie...|im read lot revie...|im reading lot re...|\n","|    2|\"This soundtrack ...| \"\"Time of the Dr...| and \"\"Chronomant...| probably the bes...| so I can't say f...|\n","|    2|I truly like this...|truly like soundt...|truli like soundt...|truli like soundt...|truly like soundt...|\n","|    2|If you've played ...|youve played game...|youv play game kn...|youv play game kn...|youve played game...|\n","|    2|I am quite sure a...|quite sure actual...|quit sure actual ...|quit sure actual ...|quite sure actual...|\n","|    1|\"This is a self-p...| unless you are i...| far away from th...|selfpublished boo...|selfpublish book ...|\n","|    2|I loved Whisper o...|loved whisper wic...|love whisper wick...|love whisper wick...|loved whisper wic...|\n","|    2|I just finished r...|finished reading ...|finish read whisp...|finish read whisp...|finished reading ...|\n","|    2|This was a easy t...|easy read book ma...|easi read book ma...|easi read book ma...|easy read book ma...|\n","|    1|A complete waste ...|complete waste ti...|complet wast time...|complet wast time...|complete waste ti...|\n","|    2|This was a great ...|great booki could...|great booki could...|great booki could...|great booki could...|\n","|    2|I thought this bo...|thought book bril...|thought book bril...|thought book bril...|thought book bril...|\n","|    1|I guess you have ...|guess romance nov...|guess romanc nove...|guess romanc nove...|guess romance nov...|\n","|    1|\"I feel I have to...| that I decided t...| believe that the...|feel write keep o...|feel write keep o...|\n","|    1|It's glaringly ob...|glaringly obvious...|glaringli obviou ...|glare obvious glo...|glaringly obvious...|\n","|    2|\"When you hear fo...| three hysterical...| often comical bu...| interaction with...| that salsa sound...|\n","|    2|After I had a DVT...|dvt doctor requir...|dvt doctor requir...|dvt doctor requir...|dvt doctor requir...|\n","|    2|Excellent product...|excellent product...|excel product how...|excel product how...|excellent product...|\n","|    1|sizes are much sm...|sizes much smalle...|size much smaller...|size much smaller...|size much smaller...|\n","+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xnJirt68m_IN"},"source":["from pyspark.ml import Pipeline\n","pipeline = Pipeline(stages=stages)\n","data = pipeline.fit(datafix).transform(datafix)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2sGQw68_3P_","executionInfo":{"status":"ok","timestamp":1622363359810,"user_tz":-480,"elapsed":1009,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"e18af189-713b-4191-b431-791476700e2d"},"source":["data = datafix.drop(\"reviewText\").drop(\"label\").drop(\"tokens\").drop(\"token_features\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------------------+-----+--------------------+--------------------+--------------------+------------+\n","|          reviewText|label|              tokens|      token_features|            features|labelpredict|\n","+--------------------+-----+--------------------+--------------------+--------------------+------------+\n","|This sound track ...|    2|[this, sound, tra...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|I'm reading a lot...|    2|[i'm, reading, a,...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|\"This soundtrack ...|    2|[\"this, soundtrac...|(226435,[0,1,4,5,...|(226435,[0,1,4,5,...|         0.0|\n","|I truly like this...|    2|[i, truly, like, ...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|If you've played ...|    2|[if, you've, play...|(226435,[0,1,3,5,...|(226435,[0,1,3,5,...|         0.0|\n","|I am quite sure a...|    2|[i, am, quite, su...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|\"This is a self-p...|    1|[\"this, is, a, se...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         1.0|\n","|I loved Whisper o...|    2|[i, loved, whispe...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|I just finished r...|    2|[i, just, finishe...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|This was a easy t...|    2|[this, was, a, ea...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|A complete waste ...|    1|[a, complete, was...|(226435,[1,2,3,4,...|(226435,[1,2,3,4,...|         1.0|\n","|This was a great ...|    2|[this, was, a, gr...|(226435,[0,1,3,4,...|(226435,[0,1,3,4,...|         0.0|\n","|I thought this bo...|    2|[i, thought, this...|(226435,[0,1,2,4,...|(226435,[0,1,2,4,...|         0.0|\n","|I guess you have ...|    1|[i, guess, you, h...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         1.0|\n","|\"I feel I have to...|    1|[\"i, feel, i, hav...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         1.0|\n","|It's glaringly ob...|    1|[it's, glaringly,...|(226435,[0,1,5,7,...|(226435,[0,1,5,7,...|         1.0|\n","|\"When you hear fo...|    2|[\"when, you, hear...|(226435,[0,3,6,7,...|(226435,[0,3,6,7,...|         0.0|\n","|After I had a DVT...|    2|[after, i, had, a...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|Excellent product...|    2|[excellent, produ...|(226435,[0,1,2,3,...|(226435,[0,1,2,3,...|         0.0|\n","|sizes are much sm...|    1|[sizes, are, much...|(226435,[0,1,2,4,...|(226435,[0,1,2,4,...|         1.0|\n","+--------------------+-----+--------------------+--------------------+--------------------+------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TNEO7z0lVkUE"},"source":["newDF = data.drop(\"reviewText\").drop(\"label\").drop(\"tokens\").drop(\"token_features\")\n","dataSpark = newDF.select(\"features\",\"labelpredict\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlHmJrcqpz8_","executionInfo":{"status":"ok","timestamp":1622362911661,"user_tz":-480,"elapsed":842,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"459b0b32-2dc0-4f78-a2e3-4e4cc2e9e5f4"},"source":["print(newDF.show())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+------------+--------------------+\n","|labelpredict|            features|\n","+------------+--------------------+\n","|         0.0|(226435,[0,1,2,3,...|\n","|         0.0|(226435,[0,1,2,3,...|\n","|         0.0|(226435,[0,1,4,5,...|\n","|         0.0|(226435,[0,1,2,3,...|\n","|         0.0|(226435,[0,1,3,5,...|\n","|         0.0|(226435,[0,1,2,3,...|\n","|         1.0|(226435,[0,1,2,3,...|\n","|         0.0|(226435,[0,1,2,3,...|\n","|         0.0|(226435,[0,1,2,3,...|\n","|         0.0|(226435,[0,1,2,3,...|\n","|         1.0|(226435,[1,2,3,4,...|\n","|         0.0|(226435,[0,1,3,4,...|\n","|         0.0|(226435,[0,1,2,4,...|\n","|         1.0|(226435,[0,1,2,3,...|\n","|         1.0|(226435,[0,1,2,3,...|\n","|         1.0|(226435,[0,1,5,7,...|\n","|         0.0|(226435,[0,3,6,7,...|\n","|         0.0|(226435,[0,1,2,3,...|\n","|         0.0|(226435,[0,1,2,3,...|\n","|         1.0|(226435,[0,1,2,4,...|\n","+------------+--------------------+\n","only showing top 20 rows\n","\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XnZAjD-LCBJ4"},"source":["# Bagi Data Train dan Test"]},{"cell_type":"code","metadata":{"id":"ZlB5JZN6AOP5","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"error","timestamp":1622370263879,"user_tz":-480,"elapsed":14,"user":{"displayName":"Hasri Ainun Eka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj_gjlmLb17GlqVWnJjmfv6TLU3sdG2XDr7id896Q=s64","userId":"12108844665682009304"}},"outputId":"46f490d6-eb1a-41a4-e795-f3ffcb626a70"},"source":["# train, test = newDF.randomSplit([0.8, 0.], seed = 2018)\n","train, test = newDF.randomSplit([0.8, 0.])"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7f19b0112ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train, test = newDF.randomSplit([0.8, 0.], seed = 2018)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'newDF' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"vHydTGYKAQXb"},"source":["# Naive Bayes"]},{"cell_type":"code","metadata":{"id":"zPryfugzsonL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622354189196,"user_tz":-480,"elapsed":114945,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"9d3447d7-82d6-430f-b2fa-2509aee2a3fa"},"source":["from pyspark.ml.classification import NaiveBayes\n","# Initialise the model\n","nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n","# Fit the model\n","model = nb.fit(train)\n","# Make predictions on test data\n","NBpredictions = model.transform(test)\n","NBpredictions.select(\"features\", \"labelnew\", \"probability\").show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+--------+----------+--------------------+\n","|labelnew|prediction|         probability|\n","+--------+----------+--------------------+\n","|     0.0|       0.0|[0.99992733889387...|\n","|     0.0|       1.0|[0.00715290898721...|\n","|     0.0|       1.0|[1.88133976674918...|\n","|     1.0|       1.0|[0.45302600075835...|\n","|     0.0|       1.0|[5.20235844022676...|\n","|     0.0|       1.0|[0.01316379916471...|\n","|     1.0|       1.0|[0.09898257872793...|\n","|     1.0|       1.0|[0.01664370077268...|\n","|     0.0|       1.0|[0.39148136961819...|\n","|     1.0|       1.0|[0.45571448426457...|\n","|     1.0|       0.0|[0.65698762839194...|\n","|     1.0|       1.0|[0.38443304774751...|\n","|     0.0|       1.0|[4.63150617133155...|\n","|     0.0|       1.0|[0.22612424007475...|\n","|     0.0|       1.0|[0.13615854042327...|\n","|     1.0|       0.0|[0.88102546893807...|\n","|     0.0|       1.0|[0.01473924880637...|\n","|     0.0|       1.0|[0.00553262840617...|\n","|     0.0|       1.0|[0.03133216010516...|\n","|     0.0|       1.0|[0.01247002391003...|\n","+--------+----------+--------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_xXJsAgJAVkw"},"source":["# Logistic Regression"]},{"cell_type":"code","metadata":{"id":"ZlRXLEzuAX0S"},"source":["from pyspark.ml.classification import LogisticRegression\n","\n","# Initialise the model\n","Lr = LogisticRegression(labelCol=\"labelnew\", featuresCol=\"features\", maxIter=10,regParam=0.3)\n","# Fit the model\n","model = Lr.fit(train)\n","# Make predictions on test data\n","LRpredictions = model.transform(test)\n","LRpredictions.select(\"features\", \"labelnew\", \"probability\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lrsUVmcPAWbJ"},"source":["# Random Forest Classifier"]},{"cell_type":"code","metadata":{"id":"NrByJ4UfAYxc"},"source":["from pyspark.ml.classification import RandomForestClassifier\n","\n","# Initialise the model\n","Rfc = RandomForestClassifier(numTrees=3, maxDepth=5,seed=42,labelCol=\"label\",featuresCol=\"features\")\n","# Fit the model\n","model = Rfc.fit(train)\n","# Make predictions on test data\n","RFCpredictions = model.transform(test)\n","RFCpredictions.select(\"features\", \"labelnew\", \"probability\").show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eUIqVlroAdDO"},"source":["# Evaluasi Akurasi Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sf6W7XCJtIpt","executionInfo":{"status":"ok","timestamp":1622354514338,"user_tz":-480,"elapsed":43765,"user":{"displayName":"Muhammad Baso Adrian Ibrahim","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0QYbfhBw9TbevUeg4YjVy2Zj4Hq1jbMjsZIyaYA=s64","userId":"08297597282598574534"}},"outputId":"0ef75b4f-e2c2-4ebb-9246-a903968d33c9"},"source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n","accuracy = evaluator.evaluate(predictions)\n","print (\"Model Accuracy: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model Accuracy:  1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jWkcK6Gw3KR1"},"source":[""],"execution_count":null,"outputs":[]}]}